{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Author**\n",
    "Shivansh Gupta"
   ],
   "id": "f5482ad272e8f41b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Importing All the Modules**\n",
    "\n",
    "- **`YOLO` (from `ultralytics`)**\n",
    "  - Load and run the **YOLOv8 model**.\n",
    "\n",
    "- **`cv2`**\n",
    "  - Read/write images.\n",
    "  - Draw bounding boxes and other shapes.\n",
    "  - Perform color space conversions (e.g., BGR â†” RGB).\n",
    "\n",
    "- **`numpy as np`**\n",
    "  - Work with image arrays.\n",
    "  - Perform numeric and matrix operations efficiently.\n",
    "\n",
    "- **`typing.List` / `typing.Dict`**\n",
    "  - Add **type annotations** for better code clarity and readability.\n",
    "\n",
    "- **`torch`**\n",
    "  - Detect GPU availability using `torch.cuda.is_available()`.\n",
    "  - Perform tensor operations if needed.\n",
    "\n",
    "- **`os`**\n",
    "  - Read environment variables.\n",
    "  - Check file paths and manage the filesystem."
   ],
   "id": "3f8fe2d6e80e0435"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:49.910976Z",
     "start_time": "2025-09-26T16:52:46.883622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import torch"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "```python\n",
    "# Load environment variables from a .env file\n"
   ],
   "id": "5003ceafa8dd77bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:49.954136Z",
     "start_time": "2025-09-26T16:52:49.931316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"D:\\PycharmProjects\\Eco_Vision\\Backend\\.env\")"
   ],
   "id": "2fc2c6d3bcc58c95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11488\\2373333911.py:3: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  load_dotenv(dotenv_path=\"D:\\PycharmProjects\\Eco_Vision\\Backend\\.env\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### YOLO Model Setup\n",
    "\n",
    "- **Device Selection:**\n",
    "  You can **force the use of CPU or GPU** manually.\n",
    "  If not specified, the device will be **auto-detected** later.\n",
    "\n",
    "- **Model Path (`model_path`):**\n",
    "  - Uses the environment variable `MODEL_PATH` if it exists.\n",
    "  - Otherwise, defaults to `\"yolov8n.pt\"`.\n",
    "\n",
    "- **`YOLO(model_path)`**:\n",
    "  - Calls the **YOLO class** from Ultralytics with the given `model_path`.\n",
    "  - Loads the **pretrained model into memory**.\n",
    "  - **Important:** The model is loaded **once in the constructor**, so you **donâ€™t need to reload it** every time you perform detection.\n"
   ],
   "id": "ccd65e4d440f98c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:50.132364Z",
     "start_time": "2025-09-26T16:52:50.018426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = os.getenv(\"MODEL_PATH\",None) #if we didn't set model , default this come \"yolo12n.pt\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLO(model_path)"
   ],
   "id": "8600596ecb2f6898",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### As here  below we can see our model is lock & loaded on cpu not gpu as i dont have didicated graphic card ðŸ’€.",
   "id": "385e0a33dc2be99a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:50.150283Z",
     "start_time": "2025-09-26T16:52:50.144828Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"[YOLODetector] Loaded model: {model_path} on {device}\")",
   "id": "80079dd8ff0c20f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YOLODetector] Loaded model: yolo12n.pt on cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **COCO 2017 Dataset Classes**\n",
    "\n",
    "- These are the object classes we will use from the **COCO 2017 dataset**.\n",
    "- The model will detect only on these classes.\n",
    "\n",
    "> **Note:** COCO 2017 has 80 classes in total, but for our task, we select a subset relevant to our need.\n",
    "\n",
    "- When a detection is made:\n",
    "  - If the **class ID** is **in** `self.reusable_classes` â†’ âœ… keep it.\n",
    "  - If the **class ID** is **not in** `self.reusable_classes` â†’ âŒ ignore it.\n",
    "\n",
    "\n",
    "This keeps the detection system **focused only on relevant items**, reducing noise from unnecessary classes.\n"
   ],
   "id": "878411499563a814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:50.226831Z",
     "start_time": "2025-09-26T16:52:50.221484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reusable_classes = {\n",
    "    39: 'bottle',\n",
    "    41: 'cup',\n",
    "    42: 'fork',\n",
    "    43: 'knife',\n",
    "    44: 'spoon',\n",
    "    45: 'bowl',\n",
    "    46: 'banana',\n",
    "    47: 'apple',\n",
    "    51: 'orange',\n",
    "    67: 'cell phone',\n",
    "    73: 'laptop',\n",
    "    76: 'keyboard',\n",
    "    84: 'book',\n",
    "}"
   ],
   "id": "57a383b053fbd35c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Model Inference**\n",
    "\n",
    "- This is where the **actual inference happens**.\n",
    "- The input image/frame is passed to the YOLO model.\n",
    "- The model runs its **forward pass** and returns detections:\n",
    "  - **Bounding boxes** (location of objects).\n",
    "  - **Class IDs** (what the object is).\n",
    "  - **Confidence scores** (how sure the model is).\n",
    "\n",
    "> **Note:** Inference = the stage where the trained model is **applied to new data** to make predictions."
   ],
   "id": "21dacc1d11813ba1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:50.294549Z",
     "start_time": "2025-09-26T16:52:50.279268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_objects(image_path: str, conf: float = 0.5, imgsz: int = 640) -> List[Dict]:\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    classes = list(reusable_classes.keys())   #Gets all the keys (class IDs like 39, 41, 42, â€¦) from the reusable classes dictionary and make a list.\n",
    "    results = model(\n",
    "    image_path,\n",
    "    conf=conf,\n",
    "    imgsz=imgsz,\n",
    "    device=device,\n",
    "    classes=classes\n",
    ")\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        if not getattr(result, \"boxes\", None):\n",
    "            continue\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])  #Itâ€™s a tensor (because YOLO is built on PyTorch)\n",
    "            confidence = float(box.conf[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            detections.append({\n",
    "                    \"class_id\": class_id,\n",
    "                    \"class_name\": reusable_classes[class_id],\n",
    "                    \"confidence\": {\n",
    "                        \"score\": round(confidence, 4),       # raw score (0..1)\n",
    "                        \"percent\": round(confidence * 100, 1)  # human-friendly %\n",
    "                    },\n",
    "                    \"bbox\": [x1, y1, x2, y2]\n",
    "                })\n",
    "    return detections"
   ],
   "id": "3e6cb4bd2eb76971",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "results is a list of Result objects (one per input image). Each contains detected boxes, class IDs, confidences, etc.",
   "id": "e249816aa930e3e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **YOLO Inference Output (Ultralytics)**\n",
    "\n",
    "When we run inference, **Ultralytics YOLO** returns a **list of `Results` objects**\n",
    "ðŸ‘‰ one `Results` object **per input image**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: What is a `Results` object?**\n",
    "Each `Results` object contains:\n",
    "- The **input image** (possibly resized).\n",
    "- All **detections** found in that image.\n",
    "- **Helper methods** (e.g., `.plot()`, `.save()`).\n",
    "\n",
    "âž¡ï¸ In short: **`result` = container for one imageâ€™s predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: What is `.boxes` inside a result?**\n",
    "- `result.boxes` â†’ an attribute of the `Results` object.\n",
    "- It is a **`Boxes` object** (Ultralyticsâ€™ custom class).\n",
    "- Stores **all bounding boxes YOLO predicted** for that image.\n",
    "- Each entry in `result.boxes` = **one detection**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: What does each box contain?**\n",
    "A single box has:\n",
    "- `.cls` â†’ predicted **class id** (e.g., `tensor([39.])`).\n",
    "- `.conf` â†’ **confidence score** (e.g., `tensor([0.872])`).\n",
    "- `.xyxy` â†’ bounding box in **[x1, y1, x2, y2]** format (absolute pixel values).\n",
    "- `.xywh` â†’ bounding box in **[x_center, y_center, width, height]** format.\n",
    "- `.data` â†’ raw tensor with all values stacked.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Example:\n",
    "If YOLO finds **3 objects** in an image, then `result.boxes` will contain **3 box objects**,\n",
    "each with its own class, confidence, and coordinates."
   ],
   "id": "d4b3cc836a774833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Lets run & see how this works**",
   "id": "daa98e3e7538a0bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:52:50.813056Z",
     "start_time": "2025-09-26T16:52:50.334650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detect = detect_objects(\"test.jpg\")\n",
    "print(detect)"
   ],
   "id": "e31218980c1c7691",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\PycharmProjects\\Eco_Vision\\Backend\\ML notebook\\test.jpg: 576x640 8 bottles, 231.6ms\n",
      "Speed: 68.2ms preprocess, 231.6ms inference, 5.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "[{'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.9479, 'percent': 94.8}, 'bbox': [190, 67, 222, 181]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.7831, 'percent': 78.3}, 'bbox': [26, 62, 51, 155]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.7175, 'percent': 71.7}, 'bbox': [45, 81, 79, 153]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.663, 'percent': 66.3}, 'bbox': [26, 62, 51, 127]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.6598, 'percent': 66.0}, 'bbox': [145, 59, 169, 156]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.6534, 'percent': 65.3}, 'bbox': [156, 61, 186, 178]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.6361, 'percent': 63.6}, 'bbox': [79, 54, 113, 184]}, {'class_id': 39, 'class_name': 'bottle', 'confidence': {'score': 0.5118, 'percent': 51.2}, 'bbox': [60, 16, 100, 101]}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We got list of classes and here it detect 8/10 bottles in our image.",
   "id": "2e433cde8990d45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **YOLO Inference Timing Flow**\n",
    "\n",
    "The YOLO output line:\n",
    "\n",
    "```\n",
    "\n",
    "test.jpg: 576x640 8 bottles, 214.9ms\n",
    "Speed: 8.5ms preprocess, 214.9ms inference, 3.8ms postprocess per image at shape (1, 3, 576, 640)\n",
    "\n",
    "```\n",
    "\n",
    "can be visualized as:\n",
    "\n",
    "```\n",
    "\n",
    "Input Image: test.jpg (576x640)\n",
    "â”‚\n",
    "â–¼\n",
    "Preprocess: 8.5ms\n",
    "\n",
    "* Load image\n",
    "* Resize & normalize\n",
    "* Convert to tensor\n",
    "* Send to GPU\n",
    "  â”‚\n",
    "  â–¼\n",
    "  Inference: 214.9ms\n",
    "* YOLO model predicts bounding boxes & class scores\n",
    "  â”‚\n",
    "  â–¼\n",
    "  Postprocess: 3.8ms\n",
    "* Non-Max Suppression (NMS)\n",
    "* Filter overlapping boxes\n",
    "* Scale boxes to original image\n",
    "  â”‚\n",
    "  â–¼\n",
    "  Output: 8 bottles detected\n",
    "  Total time: 214.9ms\n",
    "\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Preprocess:** Preparation before model runs.\n",
    "- **Inference:** Model does all predictions.\n",
    "- **Postprocess:** Refines and formats predictions.\n",
    "\n",
    "> **Note:** Most of the time is spent in **inference**, which grows with image size or model complexity.\n",
    "\n",
    "Hereâ€™s a clean Markdown snippet for your Jupyter Notebook explaining the input tensor shape:\n",
    "\n",
    "### **Input Tensor Shape**\n",
    "\n",
    "The YOLO model input tensor has the shape:\n",
    "\n",
    "```\n",
    "\n",
    "(1, 3, 576, 640)\n",
    "\n",
    "```\n",
    "\n",
    "**Breakdown:**\n",
    "\n",
    "| Dimension | Meaning |\n",
    "|-----------|---------|\n",
    "| 1         | Batch size (**one image**) |\n",
    "| 3         | Number of color channels (**RGB**) |\n",
    "| 576       | Image height in pixels |\n",
    "| 640       | Image width in pixels |\n",
    "\n",
    "> **Note:** YOLO requires input images to be resized and converted into a tensor of shape `(batch, channels, height, width)` before passing it through the model."
   ],
   "id": "cf76f08ae5563a92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:54:34.279006Z",
     "start_time": "2025-09-26T16:54:34.271169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def annotate_image(image_path: str, detections: List[Dict]) -> np.ndarray:\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Failed to read image: {image_path}\")\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det[\"bbox\"]\n",
    "        label = f\"{det['class_name']} {det['confidence']['percent']}%\"\n",
    "        cv2.rectangle(img_bgr, (x1, y1), (x2, y2), (16, 185, 129), 2)\n",
    "        cv2.putText(img_bgr, label, (x1, y1 - 6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.2,\n",
    "                    (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow(\"Annotated Image\", img_rgb)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return img_rgb"
   ],
   "id": "78c7d06249798b6e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **`cv2.imread()`**\n",
    "\n",
    "- **What:**\n",
    "  OpenCV function to **read an image from disk**.\n",
    "\n",
    "- **Why:**\n",
    "  Loads the image into a **NumPy array** in **BGR format**.\n",
    "  > Note: OpenCV uses **Blue-Green-Red (BGR) channel order** by default, not RGB.\n",
    "\n",
    "\n",
    "### **`cv2.rectangle()`**\n",
    "\n",
    "Draws a rectangle on an image typically used for **bounding boxes** around detected objects.\n",
    "\n",
    "```python\n",
    "cv2.rectangle(img_bgr, (x1, y1), (x2, y2), (16, 185, 129), 2)\n",
    "````\n",
    "\n",
    "**Breakdown of parameters:**\n",
    "\n",
    "| Parameter        | What it is                           | Why                                                            |\n",
    "| ---------------- | ------------------------------------ | -------------------------------------------------------------- |\n",
    "| `img_bgr`        | The image array (in BGR format)      | Rectangle will be drawn directly on this image                 |\n",
    "| `(x1, y1)`       | Top-left corner of the rectangle     | Defines where the rectangle starts                             |\n",
    "| `(x2, y2)`       | Bottom-right corner of the rectangle | Defines where the rectangle ends                               |\n",
    "| `(16, 185, 129)` | BGR color tuple for the rectangle    | Chooses a visible color (here, a shade of green)               |\n",
    "| `2`              | Line thickness in pixels             | Determines how thick the rectangle border appears on the image |\n",
    "\n",
    "> **Note:** OpenCV uses **BGR** order, not RGB, for colors.\n",
    "\n",
    "### **`cv2.putText()`**\n",
    "\n",
    "Draws text on an image typically used to **display the class name and confidence** above a bounding box.\n",
    "\n",
    "```python\n",
    "cv2.putText(img_bgr, label, (x1, y1 - 6),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.2,\n",
    "            (0, 0, 0), 1, cv2.LINE_AA)\n",
    "````\n",
    "\n",
    "**Breakdown of parameters:**\n",
    "\n",
    "| Parameter                  | What it is                                            | Why                                                            |\n",
    "|----------------------------| ----------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| `img_bgr`                  | The image array where text will be drawn (BGR format) | Text appears directly on this image alongside the bounding box |\n",
    "| `label`                    | Text string (e.g., `\"bottle 92.1%\"`)                  | Displays the **object name and confidence**                    |\n",
    "| `(x1, y1 - 6)`             | Bottom-left corner coordinates of the text            | Slightly above the bounding box to avoid overlap               |\n",
    "| `cv2.FONT_HERSHEY_SIMPLEX` | Predefined OpenCV font type                           | Determines the **style of the text**                           |\n",
    "| `0.2`                      | Font scale                                            | Controls **text size** (0.5 = moderately small)                |\n",
    "| `(0, 0, 0)`                | Text color in BGR (white)                             | Ensures high visibility against most backgrounds               |\n",
    "| `1`                        | Thickness of the text stroke                          | Determines how bold the text appears                           |\n",
    "| `cv2.LINE_AA`              | Anti-aliased line type                                | Smooths edges for better readability                           |\n",
    "\n",
    "> **Note:** Using anti-aliasing (`cv2.LINE_AA`) makes the text **look smoother and more professional**.\n",
    "\n",
    "### **Converting BGR to RGB**\n",
    "\n",
    "```python\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "````\n",
    "\n",
    "**Breakdown of components:**\n",
    "\n",
    "| Component           | What it is                                      | Why                                                               |\n",
    "| ------------------- | ----------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| `img_rgb`           | New variable storing the converted image        | Needed in **RGB format** for consistent display (matplotlib, web) |\n",
    "| `cv2.cvtColor()`    | OpenCV function to convert image color spaces   | Converts BGR â†’ RGB                                                |\n",
    "| `img_bgr`           | Input image in **BGR format**                   | Source image with drawn bounding boxes and text                   |\n",
    "| `cv2.COLOR_BGR2RGB` | OpenCV constant specifying BGR â†’ RGB conversion | Ensures red, green, and blue channels are correctly reordered     |\n",
    "\n",
    "> **Note:** OpenCV uses **BGR by default**, while most display libraries (matplotlib, PIL, browsers) expect **RGB**. This conversion prevents color distortion."
   ],
   "id": "55bcefd316d33ac4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now annotation is also working",
   "id": "3995458771252922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], shape=(211, 239, 3), dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12,
   "source": "annotate_image(\"test.jpg\",detect)",
   "id": "458a85d88e3af4b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (backend-env)",
   "language": "python",
   "name": "backend-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
