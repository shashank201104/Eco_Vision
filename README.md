---

# ğŸŒ± **Eco Vision â€“ FastAPI Backend**

This repository contains the backend service for **Eco Vision**, a sustainability-focused application that detects household items using a **custom-trained YOLO model** and provides **reuse or recycling tips** for each item.

The backend integrates:

* A custom YOLO model trained on 30 recyclable/reusable object classes
* A FastAPI service for inference and image annotation
* Reuse-tip mapping for sustainability guidance
* Deployment-ready configurations (Docker + Render)
* Full ML training artifacts for transparency and reproducibility

---

# ğŸŒ **Live API (Render Deployment)**

Base URL:

```
https://eco-vision-1.onrender.com
```

Interactive API documentation:

```
https://eco-vision-1.onrender.com/docs
```

The interface supports image uploads and displays detections, reuse tips, and annotated output.

---

# ğŸ“ **Project Structure**

```
fastapi/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ build.sh
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ML work/
â”‚   â”‚   best.pt
â”‚   â”‚   notebook.ipynb
â”‚   â”‚   yolo12n.pt
â”‚   â”‚   yolo12s.pt
â”‚   â”‚   test.jpg
â”‚   â”‚   test0.jpg
â”‚   â”‚   test1.jpg
â”‚   â”‚   test2.jpg
â”‚   â”‚   test3.jpg
â”‚   â”‚   test4.jpg
â”‚   â”‚   test5.jpg
â”‚   â”‚   test6.jpg
â”‚   â”‚   test7.jpeg
â”‚   â”‚   test13.jpg
â”‚   â”‚   test15.webp
â”‚   â”‚   test16.jpg
â”‚   â”‚   test17.jpg
â”‚   â”‚   test20.jpg
â”‚   â”‚   test23.jpg
â”‚   â”‚
â”‚   â””â”€â”€ dest/
â”‚       â”‚   data.yaml
â”‚       â”‚   notebook4e6102f5bd.log
â”‚       â”‚   yolo11n.pt
â”‚       â”‚   yolo12n.pt
â”‚       â”‚   yolo12s.pt
â”‚       â”‚
â”‚       â””â”€â”€ runs/
â”‚           â””â”€â”€ detect/
â”‚               â”œâ”€â”€ custom_yolo_training/
â”‚               â”‚   â”‚   args.yaml
â”‚               â”‚   â”‚   labels.jpg
â”‚               â”‚   â”‚   results.csv
â”‚               â”‚   â”‚   train_batch0.jpg
â”‚               â”‚   â”‚   train_batch1.jpg
â”‚               â”‚   â”‚   train_batch2.jpg
â”‚               â”‚   â”‚
â”‚               â”‚   â””â”€â”€ weights/
â”‚               â”‚           best.pt     â† final trained model
â”‚               â”‚           last.pt
â”‚               â”‚
â”‚               â””â”€â”€ train/
â”‚                   â”‚   args.yaml
â”‚                   â”‚   labels.jpg
â”‚                   â”‚   results.csv
â”‚                   â”‚   results.png
â”‚                   â”‚   confusion_matrix.png
â”‚                   â”‚   confusion_matrix_normalized.png
â”‚                   â”‚   BoxF1_curve.png
â”‚                   â”‚   BoxPR_curve.png
â”‚                   â”‚   BoxP_curve.png
â”‚                   â”‚   BoxR_curve.png
â”‚                   â”‚   train_batch0.jpg
â”‚                   â”‚   train_batch1.jpg
â”‚                   â”‚   train_batch2.jpg
â”‚                   â”‚   val_batch0_labels.jpg
â”‚                   â”‚   val_batch0_pred.jpg
â”‚                   â”‚   val_batch1_labels.jpg
â”‚                   â”‚   val_batch1_pred.jpg
â”‚                   â”‚   val_batch2_labels.jpg
â”‚                   â”‚   val_batch2_pred.jpg
â”‚                   â”‚
â”‚                   â””â”€â”€ weights/
â”‚                           best.pt
â”‚                           last.pt
â”‚
â””â”€â”€ app/
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ reuse_mapping.json
    â”‚
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ yolo_detector.py
    â”‚
    â”œâ”€â”€ services/
    â”‚   â””â”€â”€ detection_service.py
    â”‚
    â”œâ”€â”€ main.py
    â”‚
    â””â”€â”€ weights/
        â””â”€â”€ best.pt   â† model used by the API
```

---

# ğŸ¤– **Model Overview**

The backend uses a **custom-trained YOLO model** with 30 household object classes, such as:

* Banana
* Tomato
* Tin can
* Bottle
* Paper towel
* Milk
* Plastic bag
* Light bulb
* Toothbrush
* Snack
* Pasta
* Pastry
* Fast food
* Cake
* Hamburger

The final model is located at:

```
ML work/dest/runs/detect/custom_yolo_training/weights/best.pt
```

and is copied into:

```
app/weights/best.pt
```

for production inference.

---

# ğŸ§  **System Architecture**

### **1. Model Layer â€“ `yolo_detector.py`**

Handles:

* YOLO model loading
* Device selection (CPU/GPU)
* Object detection
* Bounding box + confidence extraction
* Image annotation

### **2. Service Layer â€“ `detection_service.py`**

Handles:

* Running YOLO inference
* Attaching reuse tips
* Formatting detection results
* Returning annotated images as numpy arrays

### **3. API Layer â€“ `main.py`**

Provides:

```
GET /health
POST /detect
```

`/detect` accepts an uploaded image, performs detection, attaches reuse tips, annotates the image, and returns JSON + Base64.

---

# âš™ï¸ **Installation & Local Development**

### Clone the repository

```bash
git clone https://github.com/shashank201104/Eco_Vision.git
cd Eco_Vision/fastapi
```

### Install dependencies

```bash
pip install -r requirements.txt
```

or:

```bash
bash build.sh
```

### Start the API

```bash
uvicorn app.main:app --reload
```

Access locally:

```
http://127.0.0.1:8000
http://127.0.0.1:8000/docs
```

---

# ğŸ“¤ **API Usage**

## **POST /detect**

### Request:

Multipart form with a single file.

### Example:

```bash
curl -X POST "https://eco-vision-1.onrender.com/detect" \
     -F "file=@test.jpg"
```

### Example Response:

```json
{
  "total_items": 2,
  "detections": [
    {
      "class_id": 15,
      "class_name": "Bottle",
      "confidence": { "score": 0.8731, "percent": 87.3 },
      "bbox": [140, 50, 310, 490],
      "reuse_tip": "Repurpose as a water bottle, planter, or storage container."
    }
  ],
  "annotated_image": "<BASE64_ENCODED_JPEG>"
}
```

The image can be displayed directly:

```js
<img src={`data:image/jpeg;base64,${annotated_image}`} />
```

---

# ğŸ§ª **Testing the API**

Test images are available in:

```
fastapi/ML work/
```

Alternatively, use the deployed Swagger interface:

```
https://eco-vision-1.onrender.com/docs
```

---

# ğŸ³ **Deployment (Render)**

### Environment Variables

In Render Dashboard â†’ Environment:

```
MODEL_PATH=app/weights/best.pt
ALLOWED_ORIGINS=https://eco-vision-lzem.onrender.com
```

### Start Command

```
uvicorn app.main:app --host 0.0.0.0 --port $PORT
```

### Build Command

Handled by the Dockerfile.

---

# ğŸ” **Updating the YOLO Model**

To update the inference model:

1. Train a new model via the notebook
2. Locate the new weights:

   ```
   ML work/dest/runs/detect/custom_yolo_training/weights/best.pt
   ```
3. Replace:

   ```
   app/weights/best.pt
   ```
4. Redeploy or restart the backend service

---

# ğŸ¯ **Purpose**

This backend serves as the inference engine for Eco Vision.
It combines:

* Object detection
* Sustainability guidance
* Visual annotation
* Easy integration with frontends

and enables the Eco Vision application to promote environmentally responsible habits.

---

# ğŸ‘¤ **Author**

Shivansh Gupta
Eco Vision â€“ ML, backend, training, and deployment.

---